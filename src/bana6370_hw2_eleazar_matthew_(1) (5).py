# -*- coding: utf-8 -*-
"""BANA6370_HW2_Eleazar_Matthew (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14zRKS7oa5ZfxtghD5okODWvUMnD0BLfN

#Part A — Notebook Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers>=4.41.2 accelerate>=0.31.0

"""# Part B — Looking Inside a Language Model

## 1. Load a causal LLM and tokenizer
#### Loading the LLM
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="cuda",
    torch_dtype="auto",
    trust_remote_code=False,
)

# Create a pipeline
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=50,
    do_sample=False,
)

"""## 2. Generate text for a business-related prompt
#### The Inputs and Outputs of a Trained Transformer LLM
"""

prompt = "Develop a concise business plan for a healthcare technology company like Epic to increase profitability, increase value, and decrease cost and expenditures while minimizing layoffs."

output = generator(prompt)

print(output[0]['generated_text'])

"""## 3. Inspect model architecture (high-level)"""

print(model)

"""## 4. Demonstrate next-token selection from logits
#### Choosing a single token from the probability distribution (sampling / decoding)
"""

prompt = "We choose to go to the moon in this decade and do the other things, not because they are easy, but because they are"

# Tokenize the input prompt
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# Tokenize the input prompt
input_ids = input_ids.to("cuda")

# Get the output of the model before the lm_head
model_output = model.model(input_ids)

# Get the output of the lm_head
lm_head_output = model.lm_head(model_output[0])

token_id = lm_head_output[0,-1].argmax(-1)
tokenizer.decode(token_id)

model_output[0].shape

lm_head_output.shape

"""## 5. Compare generation speed with and without caching
#### Speeding up generation by caching keys and values
"""

prompt = "Develop a concise business plan for a healthcare technology company like Epic to increase profitability, increase value, and decrease cost and expenditures while minimizing layoffs."

# Tokenize the input prompt
input_ids = tokenizer(prompt, return_tensors="pt").input_ids
input_ids = input_ids.to("cuda")

# Commented out IPython magic to ensure Python compatibility.
# # Generation speed with caching
# 
# %%timeit -n 1
# # Generate the text
# generation_output = model.generate(
#   input_ids=input_ids,
#   max_new_tokens=100,
#   use_cache=True
# )

# Commented out IPython magic to ensure Python compatibility.
# # Generation speed without caching
# 
# %%timeit -n 1
# # Generate the text
# generation_output = model.generate(
#   input_ids=input_ids,
#   max_new_tokens=100,
#   use_cache=False
# )

"""# Part C — Text Classification

## Data Loading & Inspection
"""

# Commented out IPython magic to ensure Python compatibility.
# # Imports
# 
# %%capture
# !pip install transformers sentence-transformers openai
# !pip install -U datasets
# 
# from datasets import load_dataset

# Load yelp_polarity

dataset_A = load_dataset("yelp_polarity")

# Inspect splits and sample records
dataset_A

# Inspect text and label fields
dataset_A["train"][0, -1]

# Prepare text and label fields

# train split
dataset_A_train_texts = dataset_A["train"]["text"] # X_Train
dataset_A_train_labels = dataset_A["train"]["label"] # y_train

# test split
dataset_A_test_texts = dataset_A["test"]["text"] # X_test
dataset_A_test_labels = dataset_A["test"]["label"] # y_test

# Define label names
dataset_A_label_names = ["Negative", "Positive"]

print(type(dataset_A_train_texts))
print(dataset_A_train_texts)
print()

"""# Classification Approaches"""

# Model depdendencies imports
from transformers import pipeline
import numpy as np
from tqdm import tqdm
from transformers.pipelines.pt_utils import KeyDataset
from sklearn.metrics import classification_report, confusion_matrix
from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics.pairwise import cosine_similarity

import matplotlib.pyplot as plt
import seaborn as sns

# Helper functions in evaluating model performance
# C: Modified from Chapter 4 to appear more robust

import numpy as np # Ensure numpy is imported for np.unique and np.concatenate
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

def evaluate(y_true, y_pred, target_names=None):
    # Identify all unique labels that are actually present in y_true and y_pred
    unique_labels = np.unique(np.concatenate((y_true, y_pred)))

    # If target_names are provided, map them to the unique labels
    # Otherwise, classification_report will use the unique labels directly
    if target_names is not None:
        # Create a dictionary to map numerical labels to their names
        # Assuming label IDs are contiguous from 0 to len(target_names)-1
        label_map = {i: name for i, name in enumerate(target_names)}
        filtered_target_names = [label_map[label_id] for label_id in unique_labels if label_id in label_map]
    else:
        filtered_target_names = None

    print(classification_report(y_true, y_pred, labels=unique_labels, target_names=filtered_target_names, zero_division=0))

    cm = confusion_matrix(y_true, y_pred, labels=unique_labels) # Pass labels to confusion_matrix as well
    plt.figure(figsize=(10, 8)) # Increase figure size for better readability of many labels
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=filtered_target_names, yticklabels=filtered_target_names)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.tight_layout()
    plt.show()

"""## Classification Approaches for Dataset A

### **Using a Task-sepcific model**

#### Approach 1: Task-Specific Classification Model
"""

from transformers import pipeline

# Path to our HF model
model_path = "cardiffnlp/twitter-roberta-base-sentiment-latest"

# Load model into pipeline
pipe = pipeline(
    model=model_path,
    tokenizer=model_path,
    return_all_scores=True,
    truncation=True, # Add truncation
    max_length=512, # Set max length
    device="cuda:0"
)

'''
Truncation and max length assigned to help with speed
'''

# Run inference
y_pred = []
for output_raw in tqdm(pipe(KeyDataset(dataset_A["test"], "text")), total=len(dataset_A["test"])):
    # The pipeline is observed to return a single dictionary for the top prediction,
    # e.g., {'label': 'LABEL_2', 'score': 0.95},
    # rather than a list of dictionaries for all scores despite return_all_scores=True.
    # Therefore, we directly use the predicted label.

    predicted_label = output_raw['label']

    # Map the predicted label to 0 (negative) or 1 (positive)
    if predicted_label == "LABEL_0": # Negative
        y_pred.append(0)
    elif predicted_label == "LABEL_2": # Positive
        y_pred.append(1)
    else: # Handle LABEL_1 (neutral). For this binary classification, we'll map neutral to 0 (negative).
        y_pred.append(0)

# The above code was modified to address certain issues with type and
# label handling which were not an issue with the older vesions of the
# libraries the Chapter 4 notebook was using

evaluate(dataset_A["test"]["label"], y_pred, dataset_A_label_names)

# simpler confusion matrix
confusion_matrix(dataset_A["test"]["label"], y_pred)

"""
Poor model performance could be attributed to mismatch between model
choice and dataset selection, for which the model was not aptly trained on.
But the choice was made in order to explore how the model generalizes,
in the same spirit the textbook uses the Twitter RoBERTa Sentiment Analysis model.

"""

print("\n--- Example Predictions: Approach 1 Task-Specific Transformer (Yelp) ---")

# We will look at the first 5 examples from the test set
# (Assuming dataset_A_label_names is defined as ["Negative", "Positive"])

for i in range(5):
    # Retrieve the raw text and the true label index
    text = dataset_A["test"]["text"][i]
    true_index = dataset_A["test"]["label"][i]

    # Retrieve the prediction we already made (stored in y_pred)
    pred_index = y_pred[i]

    # Convert indices to string labels for readability
    true_label_str = dataset_A_label_names[true_index]
    pred_label_str = dataset_A_label_names[pred_index]

    # Print the details
    print(f"Review: {text[:120]}...")  # Truncate text to 120 chars for cleaner output
    print(f"True Label: {true_label_str} | Predicted Label: {pred_label_str}")

    # Add Interpretation based on match/mismatch
    if true_index == pred_index:
        print("INTERPRETATION: Correct. The model successfully identified the sentiment keywords.")
    else:
        print("INTERPRETATION: Incorrect. The model may have struggled with sarcasm, neutrality, or mixed signals.")

    print("-" * 40)

"""### **Classification Tasks that Leverage Embeddings**

#### Supervised Classification

#### Approach 2: Embedding-Based Classification (Logistic Regression)
"""

'''
Due to the sheer size of the Yelp dataset, it does not seem practical to encode the embeddings
for all rows of the dataset. It took 20 minutes of running just to get through 7% of the batch.

Thus for the sake of this assignment, I will be subsampling the Yelp dataset to make it smaller
and managable in size.
'''
from datasets import DatasetDict

def subsample(dataset, n_train=20000, n_test=5000, seed=42):
    train = dataset["train"].shuffle(seed=seed).select(range(n_train))
    test = dataset["test"].shuffle(seed=seed).select(range(n_test))
    return DatasetDict({"train": train, "test": test})

dataset_A_small = subsample(dataset_A)

# Load model
model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')

# Convert text to embeddings

train_texts = list(dataset_A_small["train"]["text"])
test_texts  = list(dataset_A_small["test"]["text"])

train_embeddings = model.encode(train_texts, show_progress_bar=True)
test_embeddings  = model.encode(test_texts, show_progress_bar=True)

train_embeddings.shape

# Train a Logistic Regression on our train embeddings
clf = LogisticRegression(random_state=42)
clf.fit(train_embeddings, dataset_A_small["train"]["label"])

# Predict previously unseen instances
y_pred = clf.predict(test_embeddings)
evaluate(dataset_A_small["test"]["label"], y_pred)

# simpler confusion matrix
confusion_matrix(dataset_A_small["test"]["label"], y_pred)

print("\n--- Example Predictions: Approach 2 Logistic Regression (Yelp) ---")

# We inspect the first 5 examples from your 'dataset_A_small' test set
for i in range(5):
    # Retrieve the text and true label from your subsampled dataset
    text = dataset_A_small["test"]["text"][i]
    true_index = dataset_A_small["test"]["label"][i]

    # Retrieve the prediction from your 'y_pred' array
    pred_index = y_pred[i]

    # Convert to strings (assuming dataset_A_label_names = ["Negative", "Positive"])
    true_label_str = dataset_A_label_names[true_index]
    pred_label_str = dataset_A_label_names[pred_index]

    print(f"Review: {text[:120]}...")
    print(f"True: {true_label_str} | Predicted: {pred_label_str}")

    # Interpretation logic
    if true_index == pred_index:
        print("INTERPRETATION: The embedding vector correctly captured the sentiment features (semantics).")
    else:
        # Logistic Regression on embeddings sometimes fails on negation (e.g., "not bad")
        print("INTERPRETATION: MISCLASSIFICATION. The vector average might have been skewed by conflicting words.")

    print("-" * 40)

"""#### Approach 3: Centroid-Based Similarity"""

# I don't believe Centroid-Based Similarity was covered in Chapter 4 explicitly

# Compute class centroids from training embeddings
train_labels = np.array(dataset_A_small["train"]["label"])

negative_centroid = train_embeddings[train_labels == 0].mean(axis=0)
positive_centroid = train_embeddings[train_labels == 1].mean(axis=0)

centroids = np.vstack([negative_centroid, positive_centroid])

# Compute cosine similarity between test embeddings and centroids
similarities = cosine_similarity(test_embeddings, centroids)

# Pick closest centroid
y_pred = np.argmax(similarities, axis=1)

evaluate(dataset_A_small["test"]["label"], y_pred, dataset_A_label_names)
confusion_matrix(dataset_A_small["test"]["label"], y_pred)

print("\n--- Example Predictions: Approach 3 Centroid Similarity (Yelp) ---")

# We inspect the first 5 examples from your 'dataset_A_small' test set
for i in range(5):
    # Retrieve text and labels
    text = dataset_A_small["test"]["text"][i]
    true_index = dataset_A_small["test"]["label"][i]
    pred_index = y_pred[i]  # Prediction from the centroid logic

    # Convert indices to strings
    true_label_str = dataset_A_label_names[true_index]
    pred_label_str = dataset_A_label_names[pred_index]

    print(f"Review: {text[:120]}...")
    print(f"True: {true_label_str} | Predicted: {pred_label_str}")

    # Interpretation
    if true_index == pred_index:
        print("INTERPRETATION: The review's embedding vector was closer to the correct class centroid.")
    else:
        # Centroids often fail on outliers or reviews with mixed sentiment
        print("INTERPRETATION: MISCLASSIFICATION. This review might be an outlier, sitting far from the 'average' cluster.")

    print("-" * 40)

"""#### Approach 4: Zero-Shot Classification"""

# Textbook approach

# Create embeddings for our labels
label_embeddings = model.encode(["A negative review",  "A positive review"])

from sklearn.metrics.pairwise import cosine_similarity

# Find the best matching label for each document
sim_matrix = cosine_similarity(test_embeddings, label_embeddings)
y_pred = np.argmax(sim_matrix, axis=1)

evaluate(dataset_A_small["test"]["label"], y_pred, dataset_A_label_names)

print("\n--- Example Predictions: Approach 4 Label-Embedding Zero-Shot (Yelp) ---")

# We inspect the first 5 examples from your 'dataset_A_small' test set
for i in range(5):
    # Retrieve text and true label
    text = dataset_A_small["test"]["text"][i]
    true_index = dataset_A_small["test"]["label"][i]
    pred_index = y_pred[i]  # Prediction from the label-embedding logic

    # Convert indices to strings
    true_label_str = dataset_A_label_names[true_index]
    pred_label_str = dataset_A_label_names[pred_index]

    print(f"Review: {text[:120]}...")
    print(f"True: {true_label_str} | Predicted: {pred_label_str}")

    # Interpretation
    if true_index == pred_index:
        print("INTERPRETATION: The review text was semantically closer to the correct label description.")
    else:
        # Zero-shot label matching often fails on sarcasm or indirect phrasing
        print("INTERPRETATION: MISCLASSIFICATION. The semantic meaning of the text did not align with the label embedding.")

    print("-" * 40)

"""#### Approach 5: Generative Model Classification"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "google/flan-t5-base"

tokenizer = AutoTokenizer.from_pretrained(model_name)
flan_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to("cuda")

def classify_sentiment(prompt_text):
    prompt = (
        "Classify the sentiment of the following review as Positive or Negative.\n\n"
        f"Review: {prompt_text}\n\nSentiment:"
    )

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to("cuda")
    outputs = flan_model.generate(**inputs, max_new_tokens=5)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return 1 if "positive" in response.lower() else 0


# Run classification (small subset for runtime reasons)
subset_size = 1000

y_pred = []
y_true = dataset_A_small["test"]["label"][:subset_size]

for text in tqdm(dataset_A_small["test"]["text"][:subset_size]):
    y_pred.append(classify_sentiment(text))

evaluate(y_true, y_pred, dataset_A_label_names)
confusion_matrix(y_true, y_pred)

print("\n--- Example Predictions: Approach 5 Generative Model (Yelp) ---")

# We inspect the first 5 examples from the subset you just processed
for i in range(5):
    # Retrieve text and true label from the subset
    text = dataset_A_small["test"]["text"][i]
    true_index = y_true[i]  # Using the y_true list you created
    pred_index = y_pred[i]  # Using the y_pred list you created

    # Convert indices to strings
    true_label_str = dataset_A_label_names[true_index]
    pred_label_str = dataset_A_label_names[pred_index]

    print(f"Review: {text[:120]}...")
    print(f"True: {true_label_str} | Predicted: {pred_label_str}")

    # Interpretation
    if true_index == pred_index:
        print("INTERPRETATION: The LLM correctly understood the sentiment and generated the matching token.")
    else:
        # Generative models sometimes hallucinate or get confused by double negatives
        print("INTERPRETATION: MISCLASSIFICATION. The LLM might have focused on a specific negative word despite the overall positive context (or vice versa).")

    print("-" * 40)

"""# Classification Approaches for Dataset B

"""

# Load banking77
dataset_B = load_dataset("banking77")

# Inspect splits and sample records
dataset_B

# Prepare Dataset B text and labels

dataset_B_train_texts = dataset_B["train"]["text"]
dataset_B_train_labels = dataset_B["train"]["label"]

dataset_B_test_texts = dataset_B["test"]["text"]
dataset_B_test_labels = dataset_B["test"]["label"]

dataset_B_label_names = dataset_B["train"].features["label"].names

print(dataset_B_train_texts)
print(dataset_B_train_labels)
print(dataset_B_label_names)


print(type(dataset_B_train_texts))
print(type(dataset_B_train_labels))
print(type(dataset_B_label_names))

# Prepare text and label fields
dataset_B["train"][0, -1]

"""#### Approach 1: Task-Specific Classification Model"""

from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
from tqdm import tqdm

# We use a model specifically fine-tuned for Banking77
# 'mrm8488/distilroberta-finetuned-banking77' is a popular choice for this dataset
model_path_B = "mrm8488/distilroberta-finetuned-banking77"

pipe_B = pipeline(
    "text-classification",
    model=model_path_B,
    tokenizer=model_path_B,
    device=0  # Use GPU
)

print(f"Running inference with {model_path_B}...")

# Run inference on the test set
# Note: This model returns label names (e.g., "card_arrival") rather than IDs
predictions_B_task = []
for output in tqdm(pipe_B(KeyDataset(dataset_B["test"], "text"), batch_size=16), total=len(dataset_B["test"])):
    pred_label_str = output["label"]

    # Map string label back to integer ID for evaluation consistency
    if pred_label_str in dataset_B_label_names:
        predictions_B_task.append(dataset_B_label_names.index(pred_label_str))
    else:
        # Fallback if label format mismatches (rare)
        predictions_B_task.append(-1)

# Evaluate
print("\n--- Approach 1: Task-Specific Model Performance (Banking77) ---")
evaluate(dataset_B["test"]["label"], predictions_B_task, dataset_B_label_names)

print("\n--- Example Predictions: Approach 1 Task-Specific Model (Banking77) ---")

# We look at the first 5 examples from the Banking77 test set
for i in range(5):
    # Retrieve the raw text and true label index
    text = dataset_B["test"]["text"][i]
    true_index = dataset_B["test"]["label"][i]

    # Retrieve the prediction index from your 'predictions_B_task' list
    pred_index = predictions_B_task[i]

    # Convert indices to string labels for readability
    # (Assuming dataset_B_label_names is the list of 77 intent names)
    true_label_str = dataset_B_label_names[true_index]

    # Handle the rare -1 fallback case just in case
    if pred_index != -1:
        pred_label_str = dataset_B_label_names[pred_index]
    else:
        pred_label_str = "UNKNOWN_LABEL"

    print(f"Query: {text}")
    print(f"True Intent: {true_label_str}")
    print(f"Predicted:   {pred_label_str}")

    # Interpretation logic
    if true_index == pred_index:
        print("INTERPRETATION: Correct. The fine-tuned model recognized the specific domain terminology.")
    else:
        # Fine-tuned models usually fail on ambiguity between very similar classes (e.g., card_lost vs card_stolen)
        print("INTERPRETATION: MISCLASSIFICATION. The model likely struggled to distinguish between two highly similar banking intents.")

    print("-" * 50)

"""##### For this step, we use a pre-trained model from Hugging Face that has already been fine-tuned on the Banking77 dataset (unlike the Sentiment model used for Yelp).

#### Approach 2: Embedding-Based Classification (Logistic Regression)
"""

from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression

# Load embedding model (if not already loaded)
model_emb = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')

# Banking77 is relatively small (10k train), so we can use the full dataset
# or a large subset if you are time-constrained.
train_texts_B = dataset_B["train"]["text"]
test_texts_B = dataset_B["test"]["text"]

print("Encoding Dataset B Train...")
train_embeddings_B = model_emb.encode(train_texts_B, show_progress_bar=True)
print("Encoding Dataset B Test...")
test_embeddings_B = model_emb.encode(test_texts_B, show_progress_bar=True)

# Train Logistic Regression
# We increase max_iter because 77 classes can take longer to converge
clf_B = LogisticRegression(random_state=42, max_iter=1000)
clf_B.fit(train_embeddings_B, dataset_B["train"]["label"])

# Predict
y_pred_B_lr = clf_B.predict(test_embeddings_B)

print("\n--- Approach 2: Logistic Regression with Embeddings (Banking77) ---")
evaluate(dataset_B["test"]["label"], y_pred_B_lr, dataset_B_label_names)

print("\n--- Example Predictions: Approach 2 Logistic Regression (Banking77) ---")

# We look at the first 5 examples from the Banking77 test set
for i in range(5):
    # Retrieve the text and true label index
    text = dataset_B["test"]["text"][i]
    true_index = dataset_B["test"]["label"][i]

    # Retrieve the prediction from the Logistic Regression output
    pred_index = y_pred_B_lr[i]

    # Convert indices to string labels
    true_label_str = dataset_B_label_names[true_index]
    pred_label_str = dataset_B_label_names[pred_index]

    print(f"Query: {text}")
    print(f"True Intent: {true_label_str}")
    print(f"Predicted:   {pred_label_str}")

    # Interpretation logic
    if true_index == pred_index:
        print("INTERPRETATION: Correct. The embedding placed this query in the correct decision boundary.")
    else:
        # Logistic Regression on embeddings is generally robust but can fail on subtle boundaries
        print("INTERPRETATION: MISCLASSIFICATION. The query vector was likely near the boundary of a semantically similar class.")

    print("-" * 50)

"""##### We reuse the SentenceTransformer to generate embeddings. Since Banking77 is smaller than Yelp (approx 10k train vs 500k), we can likely use the full training set without aggressive subsampling, but you can subsample if it runs too slowly.

#### Approach 3: Centroid-Based Similarity
"""

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# 1. Create a DataFrame to group embeddings by label
df_B = pd.DataFrame(train_embeddings_B)
df_B['label'] = dataset_B["train"]["label"]

# 2. Calculate the mean embedding (centroid) for each of the 77 classes
centroids_B = df_B.groupby('label').mean().values

# 3. Compute cosine similarity between Test Embeddings and the 77 Centroids
sim_matrix_B = cosine_similarity(test_embeddings_B, centroids_B)

# 4. Pick the class with the highest similarity score
y_pred_B_centroid = np.argmax(sim_matrix_B, axis=1)

print("\n--- Approach 3: Centroid-Based Similarity (Banking77) ---")
evaluate(dataset_B["test"]["label"], y_pred_B_centroid, dataset_B_label_names)

print("\n--- Example Predictions: Approach 3 Centroid Similarity (Banking77) ---")

# We inspect the first 5 examples from the Banking77 test set
for i in range(5):
    # Retrieve the text and true label index
    text = dataset_B["test"]["text"][i]
    true_index = dataset_B["test"]["label"][i]

    # Retrieve the prediction index from the centroid logic
    pred_index = y_pred_B_centroid[i]

    # Convert indices to string labels
    true_label_str = dataset_B_label_names[true_index]
    pred_label_str = dataset_B_label_names[pred_index]

    print(f"Query: {text}")
    print(f"True Intent: {true_label_str}")
    print(f"Predicted:   {pred_label_str}")

    # Interpretation logic
    if true_index == pred_index:
        print("INTERPRETATION: Correct. The query vector is close to the 'center of mass' for this intent cluster.")
    else:
        # Centroids fail when a specific query is an outlier, far from the class average
        print("INTERPRETATION: MISCLASSIFICATION. This specific query might use unique phrasing that places it far from the average examples of its true class.")

    print("-" * 50)

"""##### This approach calculates the "average meaning" (centroid) of each of the 77 intents and classifies new queries by finding the closest centroid.

#### Approach 4: Zero-Shot Classification
"""

# 1. Encode the 77 label names directly
print("Encoding Label Names...")
label_embeddings_B = model_emb.encode(dataset_B_label_names)

# 2. Compute similarity between document embeddings and label embeddings
sim_matrix_B_zs = cosine_similarity(test_embeddings_B, label_embeddings_B)

# 3. Predict the label with the closest name embedding
y_pred_B_zs = np.argmax(sim_matrix_B_zs, axis=1)

print("\n--- Approach 4: Zero-Shot Classification (Banking77) ---")
evaluate(dataset_B["test"]["label"], y_pred_B_zs, dataset_B_label_names)

print("\n--- Example Predictions: Approach 4 Zero-Shot Classification (Banking77) ---")

# We inspect the first 5 examples from the Banking77 test set
for i in range(5):
    # Retrieve the raw text and true label index
    text = dataset_B["test"]["text"][i]
    true_index = dataset_B["test"]["label"][i]

    # Retrieve the prediction index from the Zero-Shot logic
    pred_index = y_pred_B_zs[i]

    # Convert indices to string labels
    true_label_str = dataset_B_label_names[true_index]
    pred_label_str = dataset_B_label_names[pred_index]

    print(f"Query: {text}")
    print(f"True Intent: {true_label_str}")
    print(f"Predicted:   {pred_label_str}")

    # Interpretation logic
    if true_index == pred_index:
        print("INTERPRETATION: Correct. The label name itself describes the query well enough for a match.")
    else:
        # Zero-shot often fails when the label name is abstract (e.g., 'transfer_timing' vs 'pending_transfer')
        print("INTERPRETATION: MISCLASSIFICATION. The semantic meaning of the label name didn't align closely enough with the query text.")

    print("-" * 50)

"""##### Here we check how well the embeddings of the label names themselves match the user queries, without training a classifier.

#### Approach 5: Generative Model Classification
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from tqdm import tqdm

# --- Step 1: Setup the Generative Model ---
model_name_gen = "google/flan-t5-base"
tokenizer_gen = AutoTokenizer.from_pretrained(model_name_gen)
model_gen = AutoModelForSeq2SeqLM.from_pretrained(model_name_gen).to("cuda")

def classify_intent_gen(prompt_text):
    prompt = (
        f"Identify the banking intent for the following query.\n\n"
        f"Query: {prompt_text}\n\n"
        "Intent:"
    )
    inputs = tokenizer_gen(prompt, return_tensors="pt", truncation=True).to("cuda")
    outputs = model_gen.generate(**inputs, max_new_tokens=10)
    return tokenizer_gen.decode(outputs[0], skip_special_tokens=True)

# --- Step 2: Generate Outputs for a Subset ---
# Generating for all 3k test items takes too long; using 200 for demonstration.
eval_size = 200
print(f"Running Generative Classification on first {eval_size} test items...")

gen_texts_B = []
# We store the true labels for this subset to compare against later
true_labels_subset = dataset_B["test"]["label"][:eval_size]

for text in tqdm(dataset_B["test"]["text"][:eval_size]):
    gen_texts_B.append(classify_intent_gen(text))

# --- Step 3: Map Free Text to Label IDs using Embeddings ---
# We need to map the model's text output (e.g., "card lost") to the specific Label ID (e.g., 42)
# We use the embedding model you loaded in Approach 2 ('model_emb')

print("Mapping generated text to label IDs...")
# Encode the text generated by the LLM
gen_embeddings_B = model_emb.encode(gen_texts_B)

# Encode the official label names (if not already done in Approach 4)
if 'label_embeddings_B' not in locals():
    label_embeddings_B = model_emb.encode(dataset_B_label_names)

# Find which official label is closest to the generated text
sim_matrix_gen = cosine_similarity(gen_embeddings_B, label_embeddings_B)
y_pred_B_gen = np.argmax(sim_matrix_gen, axis=1)

# --- Step 4: Evaluate ---
print("\n--- Approach 5: Generative Model Performance (Subset) ---")
evaluate(true_labels_subset, y_pred_B_gen, dataset_B_label_names)

print("\n--- Example Predictions: Approach 5 Generative Model (Banking77) ---")

# We inspect the first 5 examples from the subset you just processed
for i in range(5):
    # Retrieve inputs and labels from the subset variables
    text = dataset_B["test"]["text"][i]
    true_index = true_labels_subset[i]
    pred_index = y_pred_B_gen[i]

    # Retrieve the RAW text the model generated (before mapping)
    raw_output = gen_texts_B[i]

    # Convert indices to string labels
    true_label_str = dataset_B_label_names[true_index]
    pred_label_str = dataset_B_label_names[pred_index]

    print(f"Query:       {text}")
    print(f"True Intent: {true_label_str}")
    print(f"Raw LLM Out: {raw_output}")     # Important to see what Flan-T5 actually wrote
    print(f"Mapped Pred: {pred_label_str}") # The closest label we found via embeddings

    # Interpretation
    if true_index == pred_index:
        print("INTERPRETATION: Correct. The LLM generated a phrase semantically identical to the official label.")
    elif raw_output.strip() == "":
        print("INTERPRETATION: FAILURE. The model generated an empty string, leading to a random or default mapping.")
    else:
        print("INTERPRETATION: MISCLASSIFICATION. The LLM's generated phrase was too vague or mapped to a neighboring concept (e.g., 'card issue' mapping to 'card_not_working' instead of 'card_arrival').")

    print("-" * 60)