# -*- coding: utf-8 -*-
"""HW1_Eleazar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZeeVS3iN8n9Gpbx7sBYu6s00qE0CP5oN

### 1. Running an LLM for a Business Scenario
"""

# Commented out IPython magic to ensure Python compatibility.
# ## Install dependencies needed to run the following cells.
# %%capture
# !pip install transformers>=4.40.1 accelerate>=0.27.2

## We load our model onto the GPU for faster inference.
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="cuda",
    torch_dtype="auto",
    trust_remote_code=False,
)
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

## Make it easier for us to use the generator by creating a pipeline object
from transformers import pipeline

# Create a pipeline
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=500,
    do_sample=False
)

## Create prompts as a user and feed it to the model
# I will provide the prompts in 3 separate pipelines for prompt readability.

# The prompt (user input / query)
messages = [
    {"role": "user", "content": "Write an email to a potential client explaining how customer acquisition costs, churn, and lifetime value vary across DFW zip codes, and which local markets warrant increased marketing spend versus pullback."}
]

# Generate output
output = generator(messages)
print(output[0]["generated_text"])

# The prompt (user input / query)
messages = [
    {"role": "user", "content": "Suggest Dallas–Fort Worth neighborhoods that show the strongest mismatch between consumer demand and existing retail or service locations, and where should expansion or closures be prioritized to maximize ROI."}
]

# Generate output
output = generator(messages)
print(output[0]["generated_text"])

# The prompt (user input / query)
messages = [
    {"role": "user", "content": "Provide examples of which traffic patterns, commute flows, and time-of-day trends in the DFW metroplex most impact workforce productivity, and how hybrid or location strategies be adjusted to reduce costs."}
]

# Generate output
output = generator(messages)
print(output[0]["generated_text"])

"""Writeup:

An LLM can support business analysts in many ways. For one, it can automate the creation of repetitive or grunt-work text generation like emails. It can also aid in idea generation and strategy alignment. However, I can't help but notice the limitations these LLMs have on outputs: Phi-3-mini-4k-instruct lacks access to realtime-uptodate gegraphical data, which makes it impossible to give real geographical suggestions; Halllucinations also make it hard to tell the difference between real and false information generated by the LLM if you are unfamiliar with the topic you are exploring.

### 2. Tokenizing Business Text
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install --upgrade transformers==4.41.2 sentence-transformers==3.0.1 gensim==4.3.2 scikit-learn==1.5.0 accelerate==0.31.0 peft==0.11.1 scipy==1.10.1 numpy==1.26.4

# prompt chosen: Suggest Dallas–Fort Worth neighborhoods that show the strongest mismatch between consumer demand and existing retail or service locations, and where should expansion or closures be prioritized to maximize ROI.

prompt = "Suggest Dallas–Fort Worth neighborhoods that show the strongest mismatch between consumer demand and existing retail or service locations, and where should expansion or closures be prioritized to maximize ROI."

## Convert the prompt into input_ids
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to("cuda")

# Generate the text
generation_output = model.generate(
  input_ids=input_ids,
  max_new_tokens=20
)

## Print the raw input_ids
print(input_ids)

## Loop over each ID and print tokenizer.decode(id) on its own line
for id in input_ids[0]:
  print(tokenizer.decode(id))

"""Writeup:

In downstream analytics tasks such as sentiment analysis, named entity recognition, or KPI extraction, inconsistent tokenization of financial terms or locations can reduce model accuracy. This is especially important in business contexts where small numerical or semantic differences can change analytical conclusions.

For instance, it this prompt alone words like Fort Worth, prioritized, among many others were broken up in at least 2-4 tokens, and an abbreviation like ROI was even broken up into 2 tokens.

### 3. Comparing Tokenizers on Business Phrases
"""

# Borrowing show_tokens function from Chapter 2 notebook

from transformers import AutoModelForCausalLM, AutoTokenizer

colors_list = [
    '102;194;165', '252;141;98', '141;160;203',
    '231;138;195', '166;216;84', '255;217;47'
]

def show_tokens(sentence, tokenizer_name):
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    token_ids = tokenizer(sentence).input_ids
    for idx, t in enumerate(token_ids):
        print(
            f'\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +
            tokenizer.decode(t) +
            '\x1b[0m',
            end=' '
        )

text = """
In Q4, revenue increased 12.5% to $4.3M, driven by higher average
order value (+8.1%) and improved conversion rates (3.4% → 3.9%).
Gross profit margin expanded from 41.2% to 44.6% following supplier
renegotiations and reduced fulfillment costs per unit (–9.7%).
Key KPIs showed steady operational health, with customer acquisition
cost holding at $126, monthly active users up 10.2% QoQ, and on-time
delivery reaching 97.8%. However, customer churn ticked up slightly
from 4.6% to 5.1%, signaling a need to strengthen retention
initiatives despite overall profitability gains.

"""

# Compare different tokenizers

show_tokens(text, "bert-base-uncased")

show_tokens(text, "bert-base-cased")

show_tokens(text, "gpt2")

"""Writeup:

The tokenizers differ significantly in how they handle numbers, percentages, currency symbols, and compound business terms. The BERT tokenizers tend to split numbers and symbols into more granular subcomponents, while GPT-2 often treats certain numeric sequences as single tokens. These differences can affect downstream analytics such as KPI extraction or sentiment scoring, where misaligned token boundaries may cause models to miss or misinterpret key financial indicators.

### 4. Text Embeddings for Customer Feedback
"""

# Reusing SentenceTransformer from Chapter 2

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Load model
model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')

# 2. Create customer feedback sentences (8+)
feedback_sentences = [
    "Checkout was fast and easy",
    "The website is easy to navigate",
    "Customer support never replied",
    "I waited a long time for help from support",
    "The product arrived on time",
    "Packaging was fine, nothing special",
    "I love the design of the app",
    "The instructions were unclear"
]

# 3. Compute embeddings for all sentences
feedback_embeddings = model.encode(feedback_sentences)

# 4. Query sentence
query_sentence = "I am unhappy with the response time from support"
query_embedding = model.encode([query_sentence])

# Compute cosine similarity
similarities = cosine_similarity(query_embedding, feedback_embeddings)[0]

# 5. Get top 3 most similar sentences
top_indices = np.argsort(similarities)[-3:][::-1]

print("Query:", query_sentence)
print("\nTop 3 most similar feedback sentences:\n")

for idx in top_indices:
    print(f"- {feedback_sentences[idx]} (similarity: {similarities[idx]:.3f})")

"""Writeup:

Similarity between embeddings can be used in business analytics by allowing analysts to predict potential future problems or connect interrelated problems together and find holistic solutions to addreess all of them. Grouping complaints allows the organization to see which problems are most similar to see what kinds of problems the business faces the most.
Embedding similarities can also be useful to businessess in routing tickets so that they can be directed towards the right people with the right expertise in the shortest amount of time.

### 5. Recommending Business Content Using Embeddings
"""

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')

# Create Business Resources DataFrame (10 items)

data = [
    {
        "id": 1,
        "title": "Churn Analysis for Subscription Services",
        "description": "Techniques for predicting customer churn using behavioral and transaction data."
    },
    {
        "id": 2,
        "title": "Sales Forecasting with Time Series Models",
        "description": "Using historical sales data to forecast future revenue and demand."
    },
    {
        "id": 3,
        "title": "Customer Segmentation with Clustering",
        "description": "Grouping customers based on demographics and purchasing behavior."
    },
    {
        "id": 4,
        "title": "Fraud Detection in Financial Transactions",
        "description": "Identifying fraudulent transactions using machine learning models."
    },
    {
        "id": 5,
        "title": "Marketing Campaign Performance Analysis",
        "description": "Measuring the effectiveness of digital marketing campaigns."
    },
    {
        "id": 6,
        "title": "Predictive Maintenance for Manufacturing",
        "description": "Using sensor data to predict equipment failures before they occur."
    },
    {
        "id": 7,
        "title": "Customer Lifetime Value Modeling",
        "description": "Estimating long-term customer value using historical transaction data."
    },
    {
        "id": 8,
        "title": "Recommendation Systems for E-Commerce",
        "description": "Building product recommendation systems using user behavior data."
    },
    {
        "id": 9,
        "title": "Supply Chain Optimization Analytics",
        "description": "Optimizing inventory and logistics using data-driven techniques."
    },
    {
        "id": 10,
        "title": "Sentiment Analysis for Customer Feedback",
        "description": "Analyzing customer reviews and feedback using natural language processing."
    }
]

df = pd.DataFrame(data)
df

# Compute Embeddings for Descriptions
embeddings = model.encode(df["description"].tolist())

df["embedding"] = embeddings.tolist()

def recommend_resources(query, top_n=3):
    query_embedding = model.encode([query])

    similarities = cosine_similarity(
        query_embedding,
        np.vstack(df["embedding"])
    )[0]

    top_indices = np.argsort(similarities)[-top_n:][::-1]

    print(f"Query: {query}\n")
    print("Top recommended resources:\n")

    for idx in top_indices:
        print(f"- {df.iloc[idx]['title']}")
        print(f"  {df.iloc[idx]['description']}")
        print(f"  Similarity: {similarities[idx]:.3f}\n")

recommend_resources("predicting customer churn from transaction data")

"""Writeup:
This embedding-based recommender could be extended into a business knowledge system by indexing internal documentation, dashboards, and reports using semantic embeddings. Analysts could receive relevant resources automatically based on their search queries or current tasks, even if exact keywords do not match. Over time, usage data could be incorporated to personalize recommendations by role or department. This approach helps reduce knowledge silos and improves productivity across data-driven teams.
"""